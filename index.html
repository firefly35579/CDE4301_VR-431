<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>CDE4301 VR-431 Final Report</title>
  <link rel="icon" href="./favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/themes/light.css" />
  <script type="module"
    src="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/shoelace-autoloader.js"></script>

  <link rel="stylesheet" href="./index.css">

  <link rel="stylesheet" href="./components/team-member/team-member.css">
  <script type="module" src="./components/team-member/team-member.js"></script>

  <link rel="stylesheet" href="./components/table-of-content/table-of-content.css">

  <script type="module" src="./components/image/image-component.js"></script>

  <script type="module" src="./components/video/video.js"></script>

  <link rel="stylesheet" href="./components/references/references.css">

  <link rel="stylesheet" href="./components/scroll-to-top/scroll-to-top.css">
  <script src="./components/scroll-to-top/scroll-to-top.js"></script>

  <script src="./components/table-component/table-component.js"></script>

  <link href="https://unpkg.com/gridjs/dist/theme/mermaid.min.css" rel="stylesheet" />
</head>

<body>
  <div class="content">
    <h1>VR-431 Postpartum Haemorrhage VR Training Simulation</h1>

       <!-- This is the team member component use to display details about your team members -->
       <div class="team-member-wrapper">
        <team-member avatar="assets/Tristan.jpg" name="Tristan Tan" department="Mechanical Engineering"
          year="2025"></team-member>
        <team-member avatar="assets/KC.jpg" name="Kirsten Negapatan" department="Mechanical Engineering"
          year="2025"></team-member>
      </div>
  
      <!-- This is a divide from the shoelace library for aesthetic purpose -->
      <sl-divider></sl-divider>
  
      <!-- This is the table-of-content component use to define all of the link directly to each section -->
      <div class="table-of-content">
        <h2>Table of Contents</h2>
        <sl-tree>
          <sl-tree-item expanded>
            <a href="#section-header-0">0. Acknowledgement</a>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-1">1. Introduction</a>
            <sl-tree-item>
              <a href="#sub-section-1-header-1">1.1. Problem Statement</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-1-header-2">1.2. Background</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-1-header-3">1.3. Novelty and Value Proposition</a>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-2">2. Design Strategy</a>
            <sl-tree-item>
              <a href="#sub-section-2-header-1">2.1. Subject Matter Research</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-2-header-2">2.2 Development Process & Storyboarding</a>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-3">3. VR Application Development - Iteration 1</a>
            <sl-tree-item>
              <a href="#sub-section-3-header-1">3.1. Application Flow</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-3-header-2">3.2. Unity Set-Up</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-3-header-3">3.3. Ensuring Accurate-to-Life Scale</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-3-header-4">3.4. Assembling Delivery Ward Environment</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-3-header-5">3.5. App Functionability: Poke Interactable</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-3-header-6">3.6. Iteration 1 Walkthrough</a>
            </sl-tree-item>
            <sl-tree-item expanded>
              <a href="#sub-section-3-header-7">3.7. Preliminary User Testing</a>
              <sl-tree-item>
                <a href="#sub-section-3-header-7-1">3.7.1 Usability Feedback</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-3-header-7-2">3.7.2 App Design Feedback</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-3-header-7-3">3.7.3 User Study Design Feedback</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-3-header-7-4">3.7.4 Data Analysis Feedback</a>
              </sl-tree-item>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-3-header-6">3.8. Hypothesis</a>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-4">4. VR Application Development - Iteration 2</a>
            <sl-tree-item>
              <a href="#sub-section-4-header-1">4.1. Summary of Feedback and Improvements Implemented</a>
            </sl-tree-item>
            <sl-tree-item expanded>
              <a href="#sub-section-4-header-2">4.2 Improved User Study Plan</a>
              <sl-tree-item>
                <a href="#sub-section-4-header-2-1">4.2.1 Control Group Materials for Phase 1: Learn</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-4-header-2-2">4.2.2 Control Group Materials for Phase 2: Practice</a>
              </sl-tree-item>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-4-header-3">4.3. Iteration 2 Walkthrough</a>
            </sl-tree-item>
            <sl-tree-item expanded>
              <a href="#sub-section-4-header-4">4.4. User Study Round 1</a>
              <sl-tree-item>
                <a href="#sub-section-4-header-4-1">4.4.1 Usability Feedback</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-4-header-4-2">4.4.2 App Design Feedback</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-4-header-4-3">4.4.3 User Study Design Feedback</a>
              </sl-tree-item>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-5">5. VR Application Development - Iteration 3</a>
            <sl-tree-item>
              <a href="#sub-section-5-header-1">5.1. Summary of Feedback and Improvements</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-5-header-2">5.2. App Functionality: Grab Interactable</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-5-header-3">5.3. App Functionality: Adding Images</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-5-header-4">5.4. App Functionality: Adding Sound Effects</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-5-header-5">5.5. User Study Phase 3: Test – Asset Improvement</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-5-header-6">5.6. Iteration 3 Walkthrough</a>
            </sl-tree-item>
            <sl-tree-item expanded>
              <a href="#sub-section-5-header-7">5.7. User Study Round 2</a>
              <sl-tree-item>
                <a href="#sub-section-5-header-7-1">5.7.1 App Design Feedback</a>
              </sl-tree-item>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-6">6. VR Application Development - Iteration 4</a>
            <sl-tree-item>
              <a href="#sub-section-6-header-1">6.1. Summary of Feedback and Improvements</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-6-header-2">6.2. Learning Material Development: Inclusion of Common Real-Life Objects to Enhance Understanding of Scale</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-6-header-3">6.3. App Functionality: Object Collider</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-6-header-4">6.4. Revised User Study Documents</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-6-header-5">6.5. Iteration 4 Walkthrough</a>
            </sl-tree-item>
            <sl-tree-item expanded>
              <a href="#sub-section-6-header-6">6.6. Iteration 4 Feedback</a>
              <sl-tree-item>
                <a href="#sub-section-6-header-6-1">6.6.1 Usability Feedback</a>
              </sl-tree-item>
              <sl-tree-item>
                <a href="#sub-section-6-header-6-2">6.6.2 App Design Feedback</a>
              </sl-tree-item>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-7">7. Results & Analysis</a>
            <sl-tree-item>
              <a href="#sub-section-7-header-1">7.1. Results</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-7-header-2">7.2. Analysis</a>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-8">8. Discussions & Recommendations</a>
            <sl-tree-item>
              <a href="#sub-section-8-header-1">8.1. Key Findings</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-8-header-2">8.2. Project Limitations</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-8-header-3">8.3. Future Work & Recommendations</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-8-header-4">8.4. Conclusion</a>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item expanded>
            <a href="#section-header-9">9. Project Workload Distribution</a>
            <sl-tree-item>
              <a href="#sub-section-9-header-1">9.1. Kirsten Clare M. Negapatan - Contribution Summary</a>
            </sl-tree-item>
            <sl-tree-item>
              <a href="#sub-section-9-header-2">9.2. Tristan Tan Tng En - Contribution Summary</a>
            </sl-tree-item>
          </sl-tree-item>
      
          <sl-tree-item>
            <a href="#references">10. References</a>
          </sl-tree-item>

        </sl-tree>
      </div>
      <sl-divider></sl-divider>
  
      <div>
  
        <!-- This is an example of what a section might look like -->
        <div id="section-header-1">
          <h2> 0. Acknowledgement </h2>
          <p>
            We would like to express our deepest appreciation to the many people who gave us guidance throughout this year-long journey, and who provided us with the tools necessary to develop our product in a more meaningful and effective way than we would have on our own. 

Special gratitude to our project supervisor, Dr. Khoo Eng Tat, who offered stimulating and pertinent suggestions and encouragement every week, and helped us coordinate our project. Their constant encouragement and insightful suggestions helped us immensely throughout our project.

We would like to acknowledge with much appreciation the crucial role of Dr. Arundhati and Dr Abhiram Kanneganti, who gave us indispensable advice and suggestions in completing the project. 

We would like to express our deepest thanks to Liu Chang, who has assisted us immensely throughout our projects in the iDP programme and whose kind advice and encouragement helped inspire us to make the project the best it could be.

We would like to express our gratitude to Cheng Haojie, who provided us with valuable assistance and suggestions to improve our project.

We would like to express our great appreciation for Charmaine, who provided invaluable help and advice with our simulation. 

Furthermore, we would like to express our deepest thanks to all the users who took the time to participate in our user studies and provide helpful feedback.
        </p>
        </div>
      </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-1">
          <h2>1. Introduction</h2>          
        </div>
  
        <div id="sub-section-1-header-1">
          <h3>1.1. Problem Statement</h3>
          <p>
            Postpartum Hemorrhage (PPH) is one of the most critical and life-threatening obstetric emergencies encountered in clinical practice. It is traditionally defined as a blood loss of more than 500 mL following vaginal delivery or more than 1,000 mL following cesarean delivery, with severe PPH involving blood loss greater than 1,500 mL or associated hemodynamic instability (Bienstock et al., 2021). PPH accounts for approximately 27% of all maternal deaths worldwide, making it the leading direct cause of maternal mortality globally (Say et al., 2024). Each year, about 14 million women experience PPH resulting in about 70,000 maternal deaths globally (WHO, 2023). The urgency and complexity of managing PPH require that healthcare professionals, particularly those in obstetrics and gynecology, be well-trained in recognizing early signs of excessive bleeding and executing prompt interventions.
          </p>
          <p>
            In Singapore, while maternal mortality rates are low compared to global averages, PPH remains a significant cause of maternal morbidity, contributing to emergency hysterectomies, ICU admissions, and long-term complications such as anemia, infection, and psychological trauma (De Silva et al., 2021). Studies have shown that up to 5% of deliveries worldwide result in PPH, with uterine atony being the most common cause, causing approximately 70% of all cases (Wormer et al., 2024).
          </p>
          <p>
            To diagnose PPH, medical staff need to determine a quantitative measure of blood loss during the third stage of labour (WHO, 2023).  The most common method used to assess and meausre this blood loss is visual estimation (WHO, 2023). However, visual estimation is widely considered to be inaccurate because it can mis-estimate blood loss to a large degree, in particular leading to under-estimation at higher volumes of blood loss. The underestimation of blood loss can be approximately 16% to 41%, and can delay the diagnosis of PPH and the administration of life-saving interventions including uterotonic agents, uterine massage, and surgical procedures (Toledo et al.).
          </p>
        </div>
  
        <div id="sub-section-1-header-2">
          <h3>1.2. Background</h3>
          <p>
            Despite its clinical importance, training for PPH recognition and management remains limited and inconsistent across medical institutions (Akter et al., 2022). At the National University of Singapore Yong Loo Lin School of Medicine (NUS YLLSOM), students often only receive didactic training on PPH; lectures where they focus on theory (A. Kanneganti, personal communication, March 21, 2025). They do not have hands-on training for estimating blood loss, they simply learn from the textbook. Below are some pages from the textbook:
          </p>
          
          <!-- Start of the Textbook Gallery -->
          <div class="gallery-container medical-gallery">
            <button id="prev-textbook" class="gallery-button">←</button>
            <img id="gallery-image-textbook" src="assets/pg177.jpg" alt="Textbook Frame 1">
            <button id="next-textbook" class="gallery-button">→</button>
          </div>
          <p id="gallery-caption-textbook" class="gallery-caption">1 of X</p>

          <p>  
            It is difficult for these lecture sessions to replicate the dynamic, real-time progression of PPH in an actual delivery setting, nor do they provide an objective, feedback-driven platform for blood loss estimation and clinical decision-making. Furthermore, without repeated exposure or the ability to practice in varied clinical contexts, students may struggle to develop confidence and accuracy in identifying when PPH is occurring and how best to respond.
          </p>
        </div>

        <div id="sub-section-1-header-3"></div>
          <h3>1.3. Novelty and Value Proposition</h3>
          <p>
            Given the high stakes of PPH management and the limitations of current training methods, it is crucial to introduce simulation-based education that can replicate real-world obstetric emergencies. Virtual Reality (VR) offers a promising solution by enabling immersive, repeatable, and feedback-rich experiences for medical students. We propose a VR training simulation focused on blood loss estimation and early PPH detection which can expose students to a range of bleeding scenarios, allowing them to hone their observational skills, make time-sensitive decisions, and gain a deeper understanding of PPH progression and treatment protocols. By incorporating visual cues and instantaneous feedback, the VR simulation will help students learn better and develop proficiency in recognizing and managing PPH effectively. NUS medical students are also given a VR headset so access to a VR simulation to practice the relevant skills when they need will give them better exposure and increase the frequency of practice they can have, ultimately helping enhance their education. The VR simulation will also utilise to-scale assets and give users a better sense of scale and context so that they can more easily relate and internalise their learning.
          </p>
          <p>
            By improving the quality of PPH training through VR, we aim to address the global call for better maternal care and enhance the readiness of future clinicians in managing one of obstetrics’ most dangerous complications. Early recognition and effective intervention not only save lives but also reduce the risk of long-term maternal health consequences, alleviate healthcare burdens, and improve patient confidence in maternal care systems. We hope to help enhance NUS medical students’ decision-making accuracy, knowledge retention, and ultimately contribute to better clinical outcomes for maternal healthcare in Singapore.
          </p>
        </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-2">
          <h2>2. Design Strategy</h2>
          
        </div>
  
        <div id="sub-section-2-header-1">
          <h3>2.1 Subject Matter Research</h3>
          <p>
            We began our project by consulting our medical advisors at NUH, Dr. Arundhati and Dr. Abhiram, to better understand current educational workflows in the realm of obstetric emergencies. Through these discussions, we identified that the key value we could provide would be to make a simulation that helps students familiarise and gain aptitude in blood loss estimation, as it is a hard skill to hone, yet critically important.  
          </p>
          <div class="centered-figure">
            <img src="assets/Dr Abhi.jpg" alt="Dr Abhi" class="centered-image" />
            <div class="image-caption">Figure 1: Discussion and Clarification with Dr. Abhiram.</div>
          </div>
          <p>
            We also chose PPH because it is a universal problem, unlike episiotomy (which we previously worked on) which is a procedure some countries perform while others do not. Thus, the value we could provide would not be limited to regional practices but instead be useful for any obstetric practitioners or institutions.
          </p>
          
        </div>
  
        <div id="sub-section-2-header-2">
          <h3>2.2 Development Process & Storyboarding</h3>
          <p>
            We had previously conducted field studies with medical students by assisting with postings conducted by NUS YLLSOM in partnership with NUS IDP. Even though it did not provide us with direct feedback on our own project, by running these observational studies, we gleaned valuable insights regarding how users interacted with VR and experienced for ourselves what potential shortcomings could be avoided or improved upon in our simulation.
          </p>
          <div class="centered-figure">
            <img src="assets/Postings.jpg" alt="Postings" class="centered-image" />
            <div class="image-caption">Figure 2: Conducting Observational Studies Previously during Multiple O&G Postings</div>
          </div>
          <p>
            We began the process of storyboarding to coordinate our efforts with our external collaborators. Creating the storyboard helped us explain to them what our plan was and thus receive valuable feedback.
          </p>

          <p>
            Below are pages from the PRactical Obstetric Multi-Professional (PROMPT) Course and Trainer's Manual which was used as our reference for our storyboard.
          </p>
         
          <!-- Start of the Medical Reference Gallery -->
          <div class="gallery-container medical-gallery">
            <button id="prev-medical" class="gallery-button">←</button>
            <img id="gallery-image-medical" src="assets/pg178.jpg" alt="Medical Reference Frame 1">
            <button id="next-medical" class="gallery-button">→</button>
          </div>
          <p id="gallery-caption-medical" class="gallery-caption">1 of X</p>
  

          <p>
            Below is an image gallery showcasing the initial storyboard we made for VR PPH training in general in medical education.
          </p>
  
          <!-- Start of the Image Gallery -->
          <div class="gallery-container">
            <button id="prev" class="gallery-button">←</button>
            <img id="gallery-image" src="assets/image1.png" alt="Storyboard Frame 1">
            <button id="next" class="gallery-button">→</button>
          </div>
          <p id="gallery-caption" class="gallery-caption">1 of 46</p>
  
          <p>
            Below is an image gallery showcasing the final storyboard for our VR Simulation after narrowing it down to Blood Loss Estimation.
          </p>
                
          <div class="gallery-container">
            <button id="prev-vr" class="gallery-button">←</button>
            <img id="gallery-image-vr" src="" alt="VR Storyboard Frame" />
            <button id="next-vr" class="gallery-button">→</button>
          </div>
          <div id="gallery-caption-vr" class="gallery-caption"></div>
          
          <p>
            The team then began working with a VR artist, Charmaine, to make VR assets from the reference material Dr. Arundhati provided us and in consultation with Dr. Abhi. Below are the reference images used and the final assets made by Charmaine:
          </p>
          <!-- Reference Image Gallery -->
          <div class="gallery-container reference-gallery">
            <button id="prev-reference" class="gallery-button">←</button>
            <img id="gallery-image-reference" src="assets/Reference Image 1.jpg" alt="Reference Frame 1">
            <button id="next-reference" class="gallery-button">→</button>
          </div>
          <p id="gallery-caption-reference" class="gallery-caption">1 of 5</p>

          <div class="centered-figure">
            <img src="assets/Charmaine Final Assets.jpg" alt="Charmaine Final Assets" class="centered-image" />
            <div class="image-caption">Figure 3: Final Assets Made by VR Artist</div>
          </div>

          <p>
            While the more complex Unity code and systems were being developed, the team identified and obtained simpler assets and began putting together the actual simulation. With an actual simulation, the team began ideating and experimenting with different features.
          </p>

        </div>
      </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-3">
          <h2>3. VR Application Development – Iteration 1</h2>
        </div>
  
        <div id="sub-section-3-header-1">
          <h3>3.1 Application Flow</h3>
          <p>
            The application was initially structured such that each interaction question resided within its own Unity scene. These scenes were linked via a C# script that managed navigation using button-triggered scene transitions. However, this approach significantly increased the application’s file size and resulted in long build times – approximately ten minutes per build – which impeded rapid prototyping.
          </p>

          <div class="centered-figure">
            <img src="assets/Build Settings Scenes.jpg" alt="Build Settings Scenes" class="centered-image" />
            <div class="image-caption">Figure 2: The Various Scenes in Our First Project File, as Shown in the Build Settings Window.</div>
          </div>
          
          <p>
            To address this issue, we transitioned to a more efficient architecture by consolidating all interactions into a single scene. Each question was assigned a distinct canvas, and we toggled their visibility using GameObject.SetActive() in response to user input. This approach, combined with Unity’s XR Simple Interactable system, reduced the build time to approximately 30 seconds and enabled a more agile development cycle.
          </p>
          
          <div class="centered-figure">
            <img src="assets/Learner Project File Hierarchy.jpg" alt="Learner Project File Hierarchy" class="centered-image" />
            <div class="image-caption">Figure 3: The Hierarchy of Our Leaner Project File, Using a New GameObject to House Various ‘Scenes’ within the Hierarchy of a Single Unity Scene.</div>
          </div>

          <div class="centered-figure">
            <img src="assets/XR Simple Interactable.jpg" alt="XR Simple Interactable" class="centered-image" />
            <div class="image-caption">Figure 4: The Components used to enable moving from one scene to the next using XR Simple Interactable</div>
          </div>
        </div>
  
        <div id="sub-section-3-header-2">
          <h3>3.2 Unity Set-Up</h3>
          <p>
            Our initial development began with a Unity project inherited from a prior research fellow. This project, however, was burdened with numerous complex plugins and packages, leading to over 80 compilation errors. 
          </p>

          <div class="centered-figure">
            <img src="assets/3.2 1.jpg" alt="Original Project File" class="centered-image" />
            <div class="image-caption">Figure 5: The Original Project File with 80+ Compilation Errors</div>
          </div>

          <p>
            After consulting with researchers at the Immersive Reality Lab, we followed a recommended clean setup process outlined in a set of slides, creating a fresh Unity project with only essential packages and plugins. This approach greatly simplified debugging and streamlined subsequent development.
          </p>
        </div>

        <div id="sub-section-3-header-3">
          <h3>3.3 Ensuring Accurate-to-Life Scale</h3>
          <p>
            To maintain realism, we benchmarked our VR environment against the Apartment Kit sample scene, which had already been calibrated for accurate real-world scaling. 
          </p>

          <div class="centered-figure">
            <img src="assets/3.3 1.png" alt="Apartment Kit" class="centered-image" />
            <div class="image-caption">Figure 6: The Apartment Kit sample environment used to scale our own app.</div>
          </div>

          <p>
            We calibrated our assets accordingly, referencing physical items such as counters available in our homes, to ensure immersive and believable spatial proportions.
          </p>
        </div>

        <div id="sub-section-3-header-4">
          <h3>3.4 Assembling Delivery Ward Environment</h3>
          <p>
            The environment design prioritised a balance between realism and clarity. We used assets commissioned by previous student teams to assemble a scene that closely resembled a delivery ward while remaining visually uncluttered. This allowed users to focus on the training objective without unnecessary distractions
          </p>

          <div class="centered-figure">
            <img src="assets/3.4 1.png" alt="Delivery Ward Environment" class="centered-image" />
            <div class="image-caption">Figure 7: The delivery ward environment created using previously-commissioned assets. </div>
          </div>
        </div>

        <div id="sub-section-3-header-5">
          <h3>3.5 App Functionality: Poke Interactable</h3>
          <p>
            We adopted Unity’s XR Poke Interactable system, as demonstrated in a YouTube tutorial (Fist Full of Shrimp, 2023), to implement poke interactions. This required the configuration of a box collider, an XR Poke Filter specifying poke direction, as well as XR Simple Interactable for triggering responses (e.g., canvas transitions or sound playback).
          </p>
        </div>

        <div id="sub-section-3-header-6">
          <h3>3.6 Iteration 1 Walkthrough</h3>
          <p>
            The completed flow featured multiple canvases representing sequential questions. Users navigated through them using poke-enabled buttons. The demonstration video is as shown.
          </p>
          <video-component 
          tag="VR Video 1" 
          source="https://www.youtube.com/embed/42y3vopfRUA"
          subtitle="Iteration 1"
          ></video-component>
        </div>

        <div id="sub-section-3-header-7">
          <h3>3.7 Preliminary User Testing</h3>
          <p>
            Feedback was obtained through a pilot session with Immersive Reality Lab members Liu Chang and Haojie. Their insights informed several critical design and usability improvements.
          </p>

          <div class="centered-figure">
            <img src="assets/3.7 1.jpg" alt="Liu Chang" class="centered-image" />
            <div class="image-caption">Figure 8: Immersive reality expert, Liu Chang, trying our app during the preliminary testing round. </div>
          </div>
        </div>

        <div id="sub-section-3-header-7-1">
          <h3>3.7.1 Usability Feedback</h3>
          <video-component 
          tag="Z-fighting" 
          source="https://www.youtube.com/embed/1j4HGfP6lc4" 
          subtitle="Z-fighting Clip"
          ></video-component>
          <p>
            Z-fighting on walls needed correction. This caused the wall to glitch throughout the user’s experience, leading to visual discomfort.

            Furthermore, the resolution for the UI elements was low, leading to pixelated edges and corners.
            
            Excessive gaze movement between UI and in-scene objects reduced user comfort and focus. 
          </p>
        </div>

        <div id="sub-section-3-header-7-2">
          <h3>3.7.2 App Design Feedback</h3>
          <div class="centered-figure">
            <img src="assets/3.7.2 1.png" alt="Confusing Feedback Text" class="centered-image" />
            <div class="image-caption">Figure 9: Original confusing feedback text. </div>
          </div>
          <p>
            Feedback messaging was generic and not tailored to the type of error (e.g., over- vs underestimation of blood loss). Our users recommended context-sensitive messaging for greater educational value. 
          </p>
          <div class="centered-figure">
            <img src="assets/3.7.2 2.png" alt="Original Summary Panel" class="centered-image" />
            <div class="image-caption">Figure 10: Original summary panel shown after the user inputs their answer, showing the breakdown of assets present in the scene. </div>
          </div>
          <p>
            Our users also noted that asset icons on the summary panel were not to scale, potentially misleading users. 
          </p>
        </div>

        <div id="sub-section-3-header-7-3">
          <h3>3.7.3 User Study Design Feedback</h3>
          <p>
            Initially, due to a limited timeframe and lack of access to relevant medical students for user testing, we were faced with the issue of a small sample size for our user validation studies. Therefore, we planned to simply gather qualitative user feedback on our application, and progressively iterate on the application using this feedback.
          </p>
          <p>
            However, our study participants proposed a more structured comparison between an experimental group undergoing VR training and a control group receiving the textbook training that medical students are currently undergoing.  At the end, both control and experimental groups would undergo the same in-person test, using life-sized physical assets mimicking the appearance of the blood collection instruments. Through data collection and statistical analysis, meaningful connections or interesting trends may arise. 
          </p>
        </div>

        <div id="sub-section-3-header-7-4">
          <h3>3.7.4 Data Analysis Feedback</h3>
          <p>
            Our participants advised tracking response accuracy and completion time per question, visualised through boxplots. They recommended comparing distributions between control and experimental groups to assess learning efficacy.
          </p>
        </div>

        <div id="sub-section-3-header-8">
          <h3>3.8 Hypothesis</h3>
          <p>
            After this first round of preliminary testing, we came up with a hypothesis: participants exposed to the VR simulation would demonstrate higher accuracy and faster response times when estimating postpartum blood loss than those trained using traditional materials.
          </p>
        </div>

        <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-4">
          <h2>4. VR Application Development – Iteration 2</h2>
        </div>

        <div id="sub-section-4-header-1">
          <h3>4.1 Summary of Feedback and Improvements Implemented</h3>
        </div>
        <p>
          Due to time constraints – specifically, the user study scheduled just 12 hours after the previous feedback session – not all of their suggestions could be implemented in Iteration 2. However, these recommendations were subsequently addressed in Iteration 3. The following table outlines only those improvements that were incorporated into Iteration 2.
        </p>
        <div class="centered-figure">
          <img src="assets/4.1 1.jpg" alt="Summary of Feedback and Improvements" class="centered-image" />
        </div>

        <div id="sub-section-4-header-2">
          <h3>4.2 Improved User Study Plan</h3>
          <p>
            Our user study flow is as depicted in the following flowchart.
          </p>

          <div class="centered-figure">
            <img src="assets/4.2 1.png" alt="User Study Flow" class="centered-image" />
            <div class="image-caption">Figure 11: The flowchart used to brief our participants on what to expect during the user study.</div>
          </div>

          <p>
            Both control and experimental groups followed an identical three-phase structure: Learn → Practice → Test. The instructional materials used in the control group's "Learn" phase were carefully aligned with the content delivered in the experimental group's VR experience. Similarly, the practice activities were standardised across both groups to ensure comparability
          </p>          
        </div>
  
        <div id="sub-section-4-header-2-1">
          <h3>4.2.1 Control Group Materials for Phase 1: Learn</h3>
          <p>
            Participants in the control group received a printed document during the initial five-minute "Learn" phase.
          </p>
          <div class="centered-figure">
            <img src="assets/4.2.1 1.jpg" alt="Learn Handout 1" class="centered-image" />
          </div>
          <div class="centered-figure">
            <img src="assets/4.2.1 2.jpg" alt="Learn Handout 2" class="centered-image" />
            <div class="image-caption">Figure 12: The ‘Phase 1’ document provided to control group participants.</div>
          </div>

          <div id="sub-section-4-header-2-2">
            <h3>4.2.2 Control Group Materials for Phase 2: Practice</h3>
            <p>
              In the subsequent five-minute "Practice" phase, control group participants received a new document. Access to the original learning material was restricted to encourage recall and active practice.
            </p>
            <div class="centered-figure">
              <img src="assets/4.2.2 1.jpg" alt="Practice Handout 1" class="centered-image" />
            </div>
            <div class="centered-figure">
              <img src="assets/4.2.2 2.jpg" alt="Practice Handout 2" class="centered-image" />
            </div>
            <div class="centered-figure">
              <img src="assets/4.2.2 3.jpg" alt="Practice Handout 3" class="centered-image" />
            </div>
            <div class="centered-figure">
              <img src="assets/4.2.2 4.jpg" alt="Practice Handout 4" class="centered-image" />
            </div>
            <div class="centered-figure">
              <img src="assets/4.2.2 5.jpg" alt="Practice Handout 5" class="centered-image" />
              <div class="image-caption">Figure 13: The ‘Phase 2’ document provided to control group participants.</div>
            </div>
            


        <div id="sub-section-4-header-3">
          <h3>4.3 Iteration 2 Walkthrough</h3>
          <p>
            A video walkthrough documenting the second iteration’s functionality and flow of the application is shown.
          </p>
          <video-component 
          tag="Iteration 2" 
          source="https://www.youtube.com/embed/exoZMj_zyx4"
          subtitle="Iteration 2"
          ></video-component>
          
        </div>
  
        <div id="sub-section-4-header-4">
          <h3>4.4 User Study Round 1</h3>
          <p>
            The first round of user testing involved four university students, each 25 years old, with minimal prior exposure to VR technology and limited medical knowledge.
          </p>
          <div class="centered-figure">
            <img src="assets/4.4 1.jpg" alt="User Study Round 1" class="centered-image" />
            <div class="image-caption">Figure 14: Participants engaged in the VR simulation during the first round of user testing.</div>
          </div>
        </div>
  
        <div id="sub-section-4-header-4-1">
          <h3>4.4.1 Usability Feedback</h3>
          <p>
            Participants noted difficulty in noticing relevant assets within the virtual environment, as these were scattered around the room. They recommended implementing visual cues to direct attention more effectively.
          </p>
          <video-component 
          tag="Scattered Assets" 
          source="https://www.youtube.com/embed/7vt2oAENn8o"
          subtitle="Scattered Assets Video"
          ></video-component>
          <p>
            Several participants also reported that they skimmed the initial instructions due to excitement about using the VR technology. As a result, they mistakenly memorised the physical dimensions of the medical items rather than their blood absorption capacities. They recommended incorporating clearer reminders about what information was critical to retain, and at what stage of the simulation.
          </p>
        </div>

        <div id="sub-section-4-header-4-2">
          <h3>4.4.2 App Design Feedback</h3>
          <p>
            Some participants reported difficulty viewing assets placed on the floor, which impaired their ability to judge the items’ size. They suggested enabling the ability to pick up and inspect these assets up close to improve understanding.
          </p>
          <p>
            Additionally, participants commented that the lack of haptic feedback diminished the intuitiveness of the interface. They recommended incorporating auditory feedback – such as sound effects – to compensate for the absence of tactile responses during interactions.
          </p>
        </div>

        <div id="sub-section-4-header-4-3">
          <h3>4.4.3 User Study Design Feedback</h3>
          <p>
            Feedback indicated that the physical objects used during Phase 3 (Test) did not sufficiently resemble the digital assets presented during the Learn and Practice phases. This lack of visual similarity hindered participants’ ability to apply their learning during the test.
          </p>
          <div class="centered-figure">
            <img src="assets/4.4.3 1.jpg" alt="Original Physical Assets" class="centered-image" />
            <div class="image-caption">Figure 15: The original style of assets used for Phase 3: Test.</div>
          </div>
        </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-5">
          <h2>5. VR Application Development – Iteration 3</h2>
        </div>
  
        <div id="sub-section-5-header-1">
          <h3>5.1 Summary of Feedback and Improvements</h3>
          <p>
            In this iteration, we implemented additional improvements based on the feedback provided by Liu Chang, Haojie, and the participants from the first user study.
          </p>
          <div class="centered-figure">
            <img src="assets/5.1 1.png" alt="Summary of Feedback and Improvements Section 5." class="centered-image" />
          </div>
        </div>
        
        <div id="sub-section-5-header-2">
          <h3>5.2 App Functionality: Grab Interactable</h3>
          <div class="centered-figure">
            <img src="assets/5.2 1.png" alt="Grab Interaction Menu" class="centered-image" />
            <div class="image-caption">Figure 16: The Meta XR Tools Grab Interaction Menu.</div>
          </div>
          <p>
            A significant enhancement in this iteration was the introduction of grab functionality. This allowed users to pick up and examine assets at eye level, providing a more accurate perception of object scale. We achieved this using Meta XR Tools' Grab Interaction Building Block and Unity components such as Rigidbody, Grabbable, Grab Interactable, and Grab Hands Interactable scripts.
          </p>
          <div class="centered-figure">
            <img src="assets/5.2 2.png" alt="Configuration of the Components" class="centered-image" />
            <div class="image-caption">Figure 17: Configuration of the Components.</div>
          </div>
          <p>
            Following a tutorial from Matthew’s slides, we first tested the setup with a sample grab interactable cube. After confirming functionality, we applied the same configuration to other grabbable assets. 
          </p>
        </div>

        <div id="sub-section-5-header-3">
          <h3>5.3 App Functionality: Adding Images</h3>
          <p>
            To guide user attention more effectively, we created red arrow graphics using PowerPoint, exported as .png files with transparent backgrounds. Upon importing into Unity, we followed a YouTube tutorial (DA LAB, 2023) to set the texture type to "Sprite (2D and UI)" and integrated them into the scene as UI Image GameObjects. These arrows served as visual markers indicating areas of interest.
          </p>
          <video-component 
          tag="Arrows Added" 
          source="https://www.youtube.com/embed/bjsRRYWzpG4"
          subtitle="Arrows Added Video"
          ></video-component>
          <div class="centered-figure">
            <img src="assets/5.3 1.png" alt="Red Arrow" class="centered-image" />
            <div class="image-caption">Figure 18: The red arrow created in PowerPoint to help point out important parts of the scene.</div>
          </div>
          <div class="centered-figure">
            <img src="assets/5.3 2.png" alt="Changing Png File Texture" class="centered-image" />
            <div class="image-caption">Figure 19: Changing the .png file’s texture type to “Sprite (2D and UI)” in order for it to be added to an Image GameObject.</div>
          </div>
          <p>
            Following a tutorial from Matthew’s slides, we first tested the setup with a sample grab interactable cube. After confirming functionality, we applied the same configuration to other grabbable assets. 
          </p>
        </div>

        <div id="sub-section-5-header-4">
          <h3>5.4 App Functionality: Adding Sound Effects </h3>
          <p>
            To compensate for the absence of haptic feedback, we introduced sound effects (SFX) for common interactions. Royalty-free audio files were sourced from Pixabay and imported into Unity. 
          </p>
          <div class="centered-figure">
            <img src="assets/5.4 1.png" alt="Adding Audio Files" class="centered-image" />
            <div class="image-caption">Figure 20: Adding the audio files to the hierarchy to enable the incorporation of SFX upon button interaction.</div>
          </div>
          <p>
            Within the XR Simple Interactable component, we configured the "Interactable Events" menu to trigger AudioSource.Play() upon button selection.
          </p>
          <div class="centered-figure">
            <img src="assets/5.4 2.png" alt="Configuration of the Components" class="centered-image" />
            <div class="image-caption">Figure 21: Configuring the playing of SFX using XR Simple Interactable.</div>
          </div>
        </div>

        <div id="sub-section-5-header-5">
          <h3>5.5 User Study Phase 3: Test – Asset Improvement</h3>
          <p>
            To enhance the realism of the test environment, we replaced the red crepe paper previously used to simulate blood loss with printed images of the actual VR assets at life-size scale. This improved alignment between virtual and physical representations.
          </p>
          <div class="centered-figure">
            <img src="assets/5.5 1.jpg" alt="Printed Out Images" class="centered-image" />
            <div class="image-caption">Figure 22: The more visually accurate revised assets, created using printed-out images. </div>
          </div>
        </div>

        <div id="sub-section-5-header-6">
          <h3>5.6 Iteration 3 Walkthrough</h3>
          <video-component 
          tag="Iteration 3" 
          source="https://www.youtube.com/embed/FyXl4VxQYBI"
          subtitle="Iteration 3"
          ></video-component>
          <p>
            A walkthrough of our third iteration with all of the above improvements incorporated is as shown.
          </p>
        </div>

        <div id="sub-section-5-header-7">
          <h3>5.7 User Study Round 2</h3>
          <p>
            The second user study included five participants: two working adults (ages 49 and 59) and three students (ages 17, 20, and 24). All participants reported limited prior experience with VR and minimal medical knowledge.
          </p>
          <div class="centered-figure">
            <img src="assets/5.7.jpg" alt="User Study Round 2" class="centered-image" />
            <div class="image-caption">Figure 23: The users who took part in the second round of our user study.</div>
          </div>
        </div>

        <div id="sub-section-5-header-7-1">
          <h3>5.7.1 App Design Feedback</h3>
          <p>
            Participants appreciated being able to view life-sized assets in VR, but struggled to associate them with their real-world equivalents. They suggested including to-scale images of familiar everyday objects to provide relatable size references.
          </p>
          <p>
            Additionally, although the grabbing feature increased engagement, users stopped using it once they realised it wasn’t essential to task completion. While they acknowledged the utility of the grab function for understanding object scale (especially for distant assets like the large floor spill), they recommended designing tasks that require asset interaction to ensure meaningful use of this feature.
          </p>
        </div>

      </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-6">
          <h2>6. VR Application Development – Iteration 4</h2>
        </div>
  
        <div id="sub-section-6-header-1">
          <h3>6.1 Summary of Feedback and Improvements</h3>
          <p>
            In response to user feedback from Iteration 3, Iteration 4 introduced several refinements to improve realism, user engagement, and instructional clarity.
          </p>
          <div class="centered-figure">
            <img src="assets/6.1.jpg" alt="Feedback Table" class="centered-image" />
          </div>
        </div>

        <div id="sub-section-6-header-2">
          <h3>6.2 Learning Material Development: Inclusion of Common Real-Life Objects to Enhance Understanding of Scale</h3>
          <p>
            To support user interpretation of object scale in VR, we researched everyday objects with dimensions comparable to our VR assets. Using PowerPoint, we created vector images of these objects and integrated them into the "Learn" phase of the application.
          </p>
          <div class="centered-figure">
            <img src="assets/6.1.jpg" alt="Feedback Table" class="centered-image" />
          </div>
        </div>

  
        <div id="sub-section-6-header-3">
          <h3>6.3 App Functionality: Object Collider</h3>
          <p>
            We introduced a new object collider mechanism in the "Learn" phase. Users are required to grab each asset and touch it to its corresponding real-life object image. Upon collision, a success sound effect is triggered and a "Next" button is activated, allowing the user to proceed.
          </p>
          <p>
            This functionality was implemented using a custom C# script featuring serialized fields for easy assignment of audio files and target GameObjects. 
          </p>
          <div class="centered-figure">
            <img src="assets/6.3 1.png" alt="Custom C# Script for Audio" class="centered-image" />
            <div class="image-caption">Figure 24: The custom C# script written to play an audio clip and activate the “Next” button GameObject upon object collision.</div>
          </div>
          <p>
            Assets were tagged as "Asset" to ensure only the intended objects triggered the interaction logic.
          </p>
          <div class="centered-figure">
            <img src="assets/6.3 2.png" alt="Differentiation" class="centered-image" />
            <div class="image-caption">Figure 25: Changing the desired assets’ tags to ‘Asset’ to differentiate them from other assets and ensure that only the desired assets are able to trigger the collision effects.</div>
          </div>     
        </div>

        <div id="sub-section-6-header-4">
          <h3>6.4 Revised User Study Documents</h3>
          <p>
            To maintain content parity, the real-life object references added to the experimental group's VR simulation were also included in the control group's printed learning materials. This ensured consistency across study conditions.
          </p>
          <div class="centered-figure">
            <img src="assets/6.1.jpg" alt="Feedback Table" class="centered-image" />
            <div class="image-caption">Figure 26: The revised ‘Phase 1’ document provided to control group participants, with real object references included for better visualisation of scale.</div>
          </div>
        </div>

        <div id="sub-section-6-header-5">
          <h3>6.5 Iteration 4 Walkthrough</h3>
          <p>
            A video of the walkthrough of our fourth iteration, incorporating the above improvements, is as shown. 
          </p>
          <video-component 
          tag="Iteration 4" 
          source="https://www.youtube.com/embed/x2cibjXlC5k"
          subtitle="Iteration 4"
          ></video-component>
        </div>

        <div id="sub-section-6-header-6">
          <h3>6.6 Iteration 4 Feedback</h3>
          <p>
            User testing involved five immersive reality experts and two working adults.
          </p>
          <div class="centered-figure">
            <img src="assets/6.6.jpg" alt="Feedback Table" class="centered-image" />
            <div class="image-caption">Figure 27: Participants in the control and experiment groups during user testing for the fourth iteration of our VR simulation.</div>
          </div>
        </div>

        <div id="sub-section-6-header-6-1">
          <h3>6.6.1 Usability Feedback</h3>
          <p>
            While real-life object comparisons were helpful, some items (e.g., banana, laptop) felt out of place in the clinical context.
          </p>
          <p>
            Furthermore, some physics could be added to enhance the realism of the assets upon being grabbed. For instance, the swabs are made of soft gauze in real life. Therefore, it feels odd that they remain rigid when grabbed – it would feel more natural if they ‘crumpled’ or folded upon being grabbed. 
          </p>
          <p>
            Grabbing certain items, such as the blood spill, felt unnatural and broke immersion.
          </p>
        </div>

        <div id="sub-section-6-header-6-2">
          <h3>6.6.2 App Design Feedback</h3>
          <p>
            Participants expressed confusion over why the inco sheet (larger in surface area) absorbed less than the large swab. They suggested including an explicit explanation for immediate clarity, explaining that the inco sheet is made of a less absorbent material and therefore can absorb less than the large cotton swab despite its larger surface area. 
          </p>
          <p>
            To reduce guesswork, users proposed a slider-based input mechanism instead of the existing multiple-choice system. They also recommended randomising question order in the "Practice" phase to further reduce guesstimation.
          </p>
          <p>
            The addition of real-world scale references was praised. Users suggested that rendering these objects in 3D would further enhance comparison.
          </p>
          <p>
            Many users appreciated the fact that VR was able to offer kinesthetic practice, which helped them to improve their familiarity with the assets. The spatial aspect of VR enhanced user understanding.

The immersive environment facilitated more accurate comprehension of asset scale compared to static 2D documents.

Contextual placement of items within the virtual delivery ward (e.g., swabs on tray, spills on floor) enhanced learning.
          </p>
          <p>
            Due to time constraints, a fifth iteration was not feasible. Nonetheless, the constructive feedback gathered will inform future development and research directions.
          </p>
        </div>
      
      </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-7">
          <h2>7. Results & Analysis</h2>
        </div>
        <div id="sub-section-7-header-1">
          <h3>7.1 Results</h3>
        </div>
          <p>
            Below is our Table of Results of our User Studies:
          </p>
          <div class="centered-figure">
            <img src="assets/Results Table.jpg" alt="Results Table" class="large-image" />
          </div>
          <p>
            You may view or download the user test results below:
          </p>
  <a href="https://raw.githubusercontent.com/firefly35579/CDE4301_VR-431/main/assets/VR User Test Results.xlsx" download>Download the Excel File</a>

  <iframe src="https://view.officeapps.live.com/op/embed.aspx?src=https://raw.githubusercontent.com/firefly35579/CDE4301_VR-431/main/assets/VR%20User%20Test%20Results.xlsx"
    width="100%" height="600px" frameborder="0"></iframe>  
    
          <p>
            We managed to conduct a total of 16 user studies. The participants mainly consisted of university peers as well as researchers from the Immersive Reality Lab. The level of prior VR experience was mostly minimal besides the researchers, while the level of prior relevant medical experience was minimal for all the users, i.e. none had significant knowledge of PPH or blood loss estimation prior to the user study. The users were timed to ensure they did not take more than 5 minutes for either the learning phase or the practice phase. 
          </p>
          <p>
            To have a measure of how accurate our users were, we decided to quantify their mistakes made as a percentage of the total number of questions they were tested on, a mistake being a deviation of more than 200 mL from the correct answer. These are highlighted in red in the table. Additionally, if a response was very far off, i.e. a large mistake, they were highlighted as well. If the volume of blood loss was underestimated by 400 mL or more it was highlighted in orange, and if it was overestimated by 400 mL or more it was highlighted in yellow.
          </p>
          <p>
            A few other metrics were also calculated, namely the average answered volume of blood loss for each test question, the number of users that made a mistake for each question, as well as the number of users that made a large mistake for each question. These will be analysed later in Section 7.2.
          </p>
          <p>
            For the purposes of results analysis, the averages and percentages seen at the bottom of the table only included results from the twelve user studies after iteration 2, i.e. the first four user studies were excluded as they were not tested on the same questions as the subsequent twelve. 
          </p>
        </div>
  

  
        <div id="sub-section-7-header-2">
          <h3>7.2 Analysis</h3>
          <p>
            Although we did not have a large enough sample to draw any statistically significant conclusions, we were able to collect helpful qualitative feedback from the participants, as well as discover some interesting findings.
          </p>
          <p>
            In general, there was quite a lot of mistakes and large underestimation of blood loss, which would translate to a lot of misdiagnosis and failure to provide proper care to mothers, highlighting the importance of further training and education in blood loss estimation and PPH. 
          </p>
          <p>
            In general and on average, the Control group (users that did the textbook learning) were actually more accurate, all of them having 3 mistakes or less, and their average percentage of mistakes was 23.09% compared to the VR group (users that did the VR learning) which had an average mistake percentage of 35.68%. Although this is not ideal for us, there is much to be gained from analysing the data. We suspect that the reason for this difference is that the Control group, even though they did not have a good sense of scale from looking at the textbook images, were able to immediately associate the volumes to the respective items and focus on memorising the respective volumes of blood loss. On the other hand, the VR group, as a few users described it, were potentially “distracted” by the various features of the simulation and, although they got a better sense of the scale and context and were able to interact more with the assets, lost focus, did not fully concentrate on memorising the volumes of blood loss, and so were slightly more inaccurate on average. Although surprising, this is not an isolated phenomenon and has been observed by studies to be a potential phenomenon of VR education Xu et. al. (2022).
          </p>
          <div class="centered-figure">
            <img src="assets/7.2 1.png" alt="Xu Extract" class="centered-image" />
            <div class="image-caption">Figure 28: Extract from Xu et. al.</div>
          </div>
          <p>
            Though they were less accurate, we did observe that the users that did the VR learning answered the questions more quickly and with more confidence than the Control group, where some users really took their time to think of the answer before answering. This observation offers some promise to the value of VR in learning PPH, because as long as we can modify the simulation or study to ensure users understand the main goal is to estimate volumes of blood loss, they can gain both the benefit of accuracy as well as increased confidence.
          </p>
          <p>
            Another key finding was where the most mistakes were made. For the questions testing users on the individual assets, the greatest percentage of large mistakes were for the soaked bed and 75x75 floor spill (33.33% and 75% respectively), and for the questions testing a combination of assets, the larger the total volume of blood loss the greater the percentage of large mistakes; 16.67%, 41.67%, 41.67%, and 58.33% for the 1350 mL, 1400 mL, 1550 mL, and 1600 mL questions respectively. This seems to correlate with the findings mentioned in Section 1.1; loss of lower blood volumes is estimated more correctly than loss of higher blood volumes (Gerdessen et al., 2020).
          </p>
          <p>
            In conclusion, some interesting findings were able to be drawn from the data collected from our user studies that highlight areas where we can improve. Hopefully with future improvements and larger sample sizes more insights can be gleaned.
          </p>         
        </div>
   
      </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-8">
          <h2>8. Discussion & Recommendations</h2>
        </div>
        <div id="sub-section-8-header-1">
          <h3>8.1 Key Findings</h3>
          <p>
            Through four major development iterations and three rounds of user testing involving a diverse group of 16 participants, our study yielded several significant findings that offer insights into the effectiveness of virtual reality (VR) as a training tool for postpartum hemorrhage (PPH) blood loss estimation. These findings help highlight the benefits and limitations of VR in terms of usability, user engagement, and educational impact.
          </p>
          <h4>
            VR Significantly Enhanced Engagement and Immersion 
          </h4>
          <p>
            Across all user groups, participants consistently expressed that the immersive nature of the VR experience made the training feel more engaging and memorable than traditional methods. Many noted that they found the simulation “fun” and “exciting,” which contributed to sustained focus and motivation throughout the session. In particular, the use of life-sized assets and an environment resembling a delivery ward helped situate the learning in a realistic context. This was especially impactful for participants as they had no prior medical training — they appreciated the chance to "walk around" and explore assets in a setting that felt authentic. We observed users spending extra time practicing and trying out the features, grabbing assets and looking around to get a better sense of the environment, even when they had finished practicing the questions. This aligns with broader trends in VR education that emphasise learner engagement and experiential learning.
          </p>
          <h4>
            Visual and Kinesthetic Learning Supported Spatial Understanding 
          </h4>
          <p>
            Participants appreciated the ability to physically interact with blood loss assets — picking them up, examining them from different angles, and aligning them with everyday objects to understand their size. The addition of familiar reference objects (e.g., a banana or a laptop) helped users relate to the medical items more intuitively, enhancing their sense of scale. One participant remarked that the banana helped her remember the size of the small swab “even outside the simulation.”
          </p>
          <p>
            Moreover, users highlighted the value of contextual asset placement — for example, seeing swabs placed on a tray and blood spills on the floor provided an additional layer of realism and helped anchor their estimations in clinical scenarios.            
          </p>
          <h4>
            Misunderstood or Underutilized or Interactive Features 
          </h4>
          <p>
            Despite the introduction of grab functionality and object alignment tasks, some users admitted to skipping these steps once they realised they were not mandatory for task completion. This suggests that while interactivity can enhance learning, it must be meaningfully tied to learning outcomes to ensure full participation. Future iterations could address this by embedding more task-driven objectives that require interaction — for example, mandating inspection of certain items before users can proceed.
          </p>
          <p>
            Additionally, a few users noted that certain interactions felt unnatural. Grabbing flat or soft items like gauze swabs without any deformation broke immersion. Suggestions such as adding soft-body physics or animations that simulate the “crumpling” of soft materials could improve realism and reinforce the tactile expectations of the user. Grabbing the blood spills also felt unnatural to some users and will most likely be changed out to instead have them get closer to it or use a swab to wipe them up.
          </p>
          <h4>
            Instructional Clarity Was Crucial — and Sometimes Lacking 
          </h4>
          <p>
            An unexpected theme in feedback was the tendency for users to overlook or misinterpret the instructions provided at the start of the simulation. Some participants were so eager to begin the immersive experience that they skimmed past important information — for example, some people began to memorise asset dimensions rather than their blood absorption capacities.
          </p>
          <p>
            Participants proposed incorporating in-simulation reminders at key moments, such as prompts that reinforce what specific attributes (e.g., capacity vs. size) should be memorised. The inclusion of clearer onboarding and perhaps gamified mini-challenges may help to reinforce core learning goals in future iterations.
          </p>
          <h4>
            Misconceptions Emerged Around Unexpected Clinical Details          
          </h4>
          <p>
            Several users expressed confusion when asset behaviour did not align with their intuition. A key example was the inco sheet absorbing less blood than the large swab, despite having a larger surface area. This feedback pointed to an opportunity for real-time in-app explanations. Clarifying such clinical facts in context — for instance, noting that the inco sheet is less absorbent due to material type — could strengthen the educational validity of the simulation and prevent misconceptions from being reinforced.
          </p>
          <h4>
            VR Provided Confidence — But at Times at the Expense of Accuracy
          </h4>
          <p>
            Users who completed the VR simulation tended to answer questions more quickly and with greater confidence, even when their answers were incorrect. This was especially apparent when compared to the control group, where participants took more time deliberating and were more cautious in their responses. While this boost in decisiveness is promising for clinical readiness — where rapid judgments are often needed — it also highlights a potential risk: if VR users are not properly oriented to the task’s learning goals, they may trade precision for speed. 
          </p>
          <p>
            Encouragingly, some VR users, when given clearer instructional cues in later iterations, showed better balance between speed and accuracy. This suggests that VR’s benefits can be fully realised with minor improvements in instructional scaffolding.
          </p>
          <h4>
            VR Was Accessible and Usable Across Demographics
          </h4>
          <p>
            Despite initial concerns about the usability of VR among older users or those unfamiliar with the technology, participants across a wide range of ages (17–59 years) — including users aged 51 to 54 — successfully navigated the simulation. This was attributed in part to intuitive interaction design (e.g., poke buttons, simple canvas toggles) and visual cues such as red arrows to guide attention. One older participant commented that they “felt comfortable once inside the app,” indicating that thoughtful design can bridge generational and technological divides. Even users with minimal or no prior experience with VR technology were able to complete the simulation and interact with its features effectively, suggesting the platform’s broad applicability in educational settings.
          </p>
          <h4>
            Difficulty Estimating High Blood Volumes
          </h4>
          <p>
            Both control and experimental groups struggled to estimate higher volumes of blood loss accurately, particularly in scenarios involving the soaked bed and large floor spill. This supports existing literature which shows that visual estimation tends to become increasingly inaccurate as blood volume increases. It also underscores the need for training tools that better support scale interpretation in such scenarios.
          </p>
        </div>

        <div id="sub-section-8-header-2">
          <h3>8.2 Project Limitations</h3>
          <p>
            Despite the potential of virtual reality (VR) as an innovative educational tool for postpartum hemorrhage (PPH) training, this project has several limitations that should be acknowledged:
          </p>
          <h4>
            Limited Clinical Realism
          </h4>
          <p>
            While VR allows for immersive simulations and we strived to make our simulation, particularly the blood loss assets, as accurate as possible, the environment and models may not fully replicate the immersion of a real-life delivery ward. Additionally, the simulation may not fully replicate the tactile sensations and nuanced decision-making under pressure experienced in real-life clinical settings. This could be a factor in the transferability of skills learned in VR to actual practice.
          </p>
          <h4>
            Technology Accessibility and Usability
          </h4>
          <p>
            Although medical students have access to VR headsets and compatible devices, if we were to cater the simulation to other medical staff such as midwives, their access to VR headsets and compatible devices may be limited. Additionally, varying levels of user familiarity with VR could influence engagement and learning outcomes. However, from our previous experiences developing VR simulations we have strived to make our simulation as user-friendly as possible so that people of all demographics and experience are able to use it. From our user studies, people of different demographics such as adults over 45 years old with minimal VR experience were still able to complete the simulation without issue.
          </p>
          <h4>
            Narrow Scope of Simulation
          </h4>
          <p>
            Even though we had done research and initially prepared a story for a simulation for the entire process of PPH, including steps to take after diagnosis, our final product focused on blood loss estimation. Thus, the simulation is limited in helping users understand the full scope of PPH management.
          </p>
          <h4>
            Evaluation of Learning Outcomes
          </h4>
          <p>
            The project may not include long-term assessments to evaluate knowledge retention, clinical performance, or real-world impact. Without rigorous outcome data, it is difficult to determine the true educational value of the VR tool.
          </p>
          <h4>
            Lack of Multidisciplinary Integration
          </h4>
          <p>
            PPH management often requires a coordinated, team-based approach. If the simulation focuses solely on individual performance, it may not reflect the collaborative dynamics of actual obstetric emergencies involving midwives, anesthesiologists, and nurses.
          </p>
        </div>

        <div id="sub-section-8-header-3">
          <h3>8.3 Future Work & Recommendation</h3>
          <p>
            Given our findings and the limitations of the current project, several areas of future work and recommendations to improve both the simulation and its educational outcomes were outlined:
          </p>
          <h4>
            Improve Instructional Design Within VR
          </h4>
          <p>
            Future iterations should place clearer emphasis on the learning objective – namely, accurate blood volume estimation. Techniques such as spaced repetition, in-simulation quizzes, and scaffolded feedback may help reinforce memorisation while retaining the strengths of VR immersion.
          </p>
          <h4>
            Integrate Clinical Context and Workflow
          </h4>
          <p>
            Expanding the simulation to include the full PPH management workflow – such as uterine massage, administration of uterotonics, and escalation steps – will provide a more holistic and clinically relevant experience. This would support training beyond estimation into diagnostic and procedural decision-making.
          </p>
          <h4>
            Enhance Feedback and Assessment Features
          </h4>
          <p>
            Incorporating more detailed feedback mechanisms (e.g., visual comparison of user answer vs actual value, or a cumulative performance dashboard) can deepen reflective learning. More intuitive interfaces such as sliders or a numerical pad (instead of MCQs) and randomized task sequences may also help reduce guesswork and help provide more detailed feedback.
          </p>
          <h4>
            Conduct Longitudinal and Larger-Scale Studies
          </h4>
          <p>
            To validate long-term learning gains and retention, future research should involve medical students, so that a comparative study can be done (medical students vs laypeople). Future research should ideally also include follow-up testing over time. Randomised controlled trials could provide more definitive evidence of educational efficacy.
          </p>
          <h4>
            Explore Collaborative and Multidisciplinary Scenarios
          </h4>
          <p>
            To better reflect real clinical environments, future versions could allow for collaborative VR experiences where users work as part of a care team. Integrating scenarios involving nurses, anesthesiologists, and midwives may help train students in team-based PPH response.          
          </p>
          <h4>
            Improve Realism of Assets and Physics
          </h4>
          <p>
            Enhancing visual and physical realism – for example, allowing assets like swabs to fold or crumple upon grabbing – can further increase immersion and improve intuitive understanding of material properties.
          </p>
          <p>
            By implementing these improvements, we believe the VR simulation can evolve into a more robust educational platform that not only supports blood loss estimation but also prepares students for the fast-paced, high-stakes realities of obstetric emergencies.
          </p>

          <div id="sub-section-8-header-4">
            <h3>8.4 Conclusion</h3>
            <p>
              This project demonstrates the feasibility and potential of using virtual reality (VR) to support medical education in postpartum hemorrhage (PPH) training. By simulating realistic delivery ward environments and incorporating interactive, to-scale medical assets, our VR simulation aimed to improve medical students’ ability to estimate blood loss volumes – a critical skill in the early detection of PPH.
            </p>
            <p>
              While our findings suggest that traditional textbook learning still outperforms VR in terms of short-term estimation accuracy, VR offers unique advantages in terms of engagement and learner confidence. The results reinforce that immersive experiences can complement existing training methods, particularly when combined with focused instructional design.
            </p>
            <p>
              The project highlights the value of integrating feedback into iterative design cycles and adapting user study protocols to address real-world challenges. Ultimately, this work contributes to the ongoing exploration of VR as a scalable, accessible, and effective educational tool in medical training, with strong potential to be refined further for future impact.
            </p>
          
          </div>

      </div>
  
      <br />
      <sl-divider></sl-divider>
      <div>
        <div id="section-header-9">
          <h2>9. Project Workload Distribution</h2>
        </div>
  
        <div id="sub-section-9-header-1">
          <h3>9.1 Kirsten Clare M. Negapatan – Contribution Summary</h3>
          <p>
            Kirsten played a central role in the design and development of the VR simulation. She was responsible for creating the initial storyboard that structured the user experience and instructional flow of the simulation. Kirsten led the Unity development across all four major iterations of the application, implementing key functionalities such as poke and grab interactions, object collider code, and enabling auditory feedback via sound effects. She also designed the in-app learning materials, including visual references and real-life object comparisons to enhance scale understanding. Kirsten oversaw the user study design and documentation, and actively facilitated and conducted user testing sessions. She contributed significantly to the report by authoring Sections 3, 4, 5, and 6, as well as supporting writing in other relevant areas.
          </p>
        </div>

        <div id="sub-section-9-header-2">
          <h3>9.2 Tristan Tan Tng En – Contribution Summary</h3>
          <p>
            Tristan was the main liaison with the external collaborators on the project, namely Dr. Abhiram and Arundhati, as well as the VR artist Charmaine. He communicated and coordinated the team’s efforts to ensure clinical accuracy and asset quality. He contributed to the planning and execution of the user studies, conducting user tests and managing data collection. Tristan was also responsible for synthesising the results and leading the quantitative and qualitative analysis in Section 7. Additionally, he helped code the interim and final reports in GitHub. Tristan authored Sections 1, 2, 7, and 8 of the final report, and provided overall editorial support throughout the writing process.
          </p>
        </div>

      </div>
  
      <br />
      <sl-divider></sl-divider>
  
      <div id="references" class="references">
        <sl-divider></sl-divider>
        <h2>10. References</h2>
        <ul>
          <li>
            Bienstock, J. L., Eke, A. C., & Hueppchen, N. A. (2021, April 29). Postpartum hemorrhage. <i>The New England Journal of Medicine</i>. 
            <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10181876/" target="_blank">https://pmc.ncbi.nlm.nih.gov/articles/PMC10181876/</a>
          </li>
          <li>
            Say, L., Chou, D., Gemmill, A., Tunçalp, Ö., Moller, A.-B., Daniels, J., Gülmezoglu, A. M., Temmerman, M., & Alkema, L. (2024, June). 
            Global causes of maternal death: a WHO systematic analysis. <i>The Lancet Global Health</i>. 
            <a href="http://www.thelancet.com/journals/langlo/article/PIIS2214-109X(14)70227-X/fulltext?rss=yes" target="_blank">http://www.thelancet.com/journals/langlo/article/PIIS2214-109X(14)70227-X/fulltext?rss=yes</a>
          </li>
          <li>
            Farrar, J., & Aylward, B. (2023, October 11). Postpartum haemorrhage. <i>World Health Organization</i>. 
            <a href="https://www.who.int/teams/sexual-and-reproductive-health-and-research-(srh)/areas-of-work/maternal-and-perinatal-health/postpartum-haemorrhage" target="_blank">
              https://www.who.int/.../postpartum-haemorrhage</a>
          </li>
          <li>
            De Silva, M., Panisi, L., Lindquist, A., Cluver, C., Middleton, A., Koete, B., Vogel, J. P., Walker, S., Tong, S., & Hastie, R. (2021, July 20). 
            Severe maternal morbidity in the Asia Pacific: A systematic review and meta-analysis. <i>BMC Pregnancy and Childbirth</i>. 
            <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8358707/" target="_blank">https://pmc.ncbi.nlm.nih.gov/articles/PMC8358707/</a>
          </li>
          <li>
            Wormer, K. C., Jamil, R. T., & Bryant, S. B. (2024, July 19). Postpartum hemorrhage. <i>StatPearls</i>. 
            <a href="https://www.ncbi.nlm.nih.gov/books/NBK499988/" target="_blank">https://www.ncbi.nlm.nih.gov/books/NBK499988/</a>
          </li>
          <li>
            World Health Organization. (2023). Methods of assessing postpartum blood loss for the detection of postpartum haemorrhage: Evidence-to-decision framework. 
            <i>WHO recommendations on the assessment of postpartum blood loss and use of a treatment bundle for postpartum haemorrhage [Internet]</i>. 
            <a href="https://www.ncbi.nlm.nih.gov/books/NBK598973/" target="_blank">https://www.ncbi.nlm.nih.gov/books/NBK598973/</a>
          </li>
          <li>
            Toledo, P., McCarthy, R. J., Hewlett, B. J., Fitzgerald, P. C., & Wong, C. A. (n.d.). The accuracy of blood loss estimation after simulated vaginal delivery. 
            <a href="https://pubmed.ncbi.nlm.nih.gov/18042876/" target="_blank">https://pubmed.ncbi.nlm.nih.gov/18042876/</a>
          </li>
          <li>
            Akter, S., Forbes, G., Miller, S., Galadanci, H., Qureshi, Z., Fawcus, S., Justus Hofmeyr, G., Moran, N., Singata-Madliki, M., Amole, T. G., Gwako, G., Osoti, A., 
            Thomas, E., Gallos, I., Mammoliti, K.-M., Coomarasamy, A., Althabe, F., Lorencatto, F., & Bohren, M. A. (2022, October 26). 
            Detection and management of postpartum haemorrhage: Qualitative evidence on healthcare providers’ knowledge and practices. 
            <i>Frontiers in Global Women's Health</i>. 
            <a href="https://www.frontiersin.org/articles/10.3389/fgwh.2022.1020163/full" target="_blank">https://www.frontiersin.org/articles/10.3389/fgwh.2022.1020163/full</a>
          </li>
          <li>
            Fist Full of Shrimp. (2023, August 4). <i>2023 Unity VR Basics – Poke Interactor</i> [Video]. YouTube. 
            <a href="https://www.youtube.com/watch?v=MaA76IHXMOM" target="_blank">https://www.youtube.com/watch?v=MaA76IHXMOM</a>
          </li>
          <li>
            DA LAB. (2023, January 4). <i>Create Sprite from Image in Unity</i> [Video]. YouTube. 
            <a href="https://www.youtube.com/watch?v=O2P3WRdtUuQ" target="_blank">https://www.youtube.com/watch?v=O2P3WRdtUuQ</a>
          </li>
          <li>
            Gerdessen, L., Meybohm, P., Choorapoikayil, S., Herrmann, E., Taeuber, I., Neef, V., Raimann, F. J., Zacharowski, K., & Piekarski, F. (2020, August 19). 
            Comparison of common perioperative blood loss estimation techniques: A systematic review and meta-analysis. 
            <i>Journal of Clinical Monitoring and Computing</i>. 
            <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7943515/" target="_blank">https://pmc.ncbi.nlm.nih.gov/articles/PMC7943515/</a>
          </li>
          <li>
            Xu, X., Pan, X., Kilroy, D., Kumar, A., Mangina, E., & Campbell, A. G. (2022). 
            Using HMD-based Hand Tracking Virtual Reality in Canine Anatomy Summative Assessment: a User Study. 
            <i>2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</i>, Singapore. 
            <a href="https://doi.org/10.1109/ISMAR55827.2022.00044" target="_blank">https://doi.org/10.1109/ISMAR55827.2022.00044</a>
          </li>
        </ul>
      </div>    

    </div>
  
    <!-- This is the code to display the scroll to top button for ergonomic -->
    <!-- You can leave it as it is, or if you don't like its aesthetics you can also just delete it, -->
    <!-- but it might reduce the user experience. -->
    <sl-button class="scroll-to-top" variant="primary" size="medium" circle onclick="scrollToTop()">
      <sl-icon name="arrow-up" label="Settings"></sl-icon>
    </sl-button>
  
    <script src="https://unpkg.com/gridjs/dist/gridjs.umd.js"></script>
    <script type="module" src="./components/table-component/table-component.js"></script>
    <script src="gallery.js"></script>
  
  </body>
  
  </html>
  